{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: General Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook covers:\n",
        "\n",
        "- Temperature\n",
        "- Max output length\n",
        "- Token counting\n"
      ],
      "metadata": {
        "id": "QOKxmDLtRogx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFTEz0qtFvxC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAK8IsGF4Os"
      },
      "source": [
        "Since we've put our gemini API key in Colab Secrets, we can just run the following cells to setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OEoeosRTv-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a642cc08-7a3c-4783-b08e-3ea5abe16b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Safety.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfoK3I3hu6V"
      },
      "source": [
        "## Model Temperature\n",
        "\n",
        "Steve is indecisive about how to spend his Friday night as a Freshman at Berkeley. He decides to ask Gemini using the 1.5 Flash [variant](https://ai.google.dev/gemini-api/docs/models/gemini):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2bcfnGEviwTI",
        "outputId": "87771730-6022-4632-b89c-af84d200bfb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grab some friends, head to the Campanile, and code a game on your laptops while enjoying the breathtaking view of the city. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJwOhbdVkD_c"
      },
      "source": [
        "Steve is very smart and knows that Gemini is non-deterministic; the same prompt can result in different outputs! To demonstrate this, the following code passes the same prompt 5 different times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxc8f6oQlCAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "bccde507-b9f8-47b5-9f56-3123dce495c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Grab some fellow CS majors and head to a hackathon at the nearby tech incubator for a night of caffeine-fueled coding and camaraderie. \n",
            "\n",
            "2. Grab some fellow CS majors and head to a nearby arcade for some retro gaming and competitive fun. \n",
            "\n",
            "3. Grab some friends, head to the bustling Sproul Plaza, and engage in a spirited debate about the latest AI breakthroughs while enjoying a late-night boba. \n",
            "\n",
            "4. Grab a slice at La Taqueria with friends and brainstorm coding projects over late-night coffee at Philz. \n",
            "\n",
            "5. Grab some friends, head to a campus pub trivia night, and test your knowledge while enjoying some cheap drinks and good company. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "outputs = []\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "for i in range(5):\n",
        "  response = model.generate_content(prompt)\n",
        "  outputs.append(response.text)\n",
        "for index, sentence in enumerate(outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA9ewE_dMFMZ"
      },
      "source": [
        "Gemini returns a different response each time. Hmm. Steve doesn't like the fact that his Friday night might be determined by random chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONFTcYStQ2YY"
      },
      "source": [
        "Fortunately, Gemini has a temperature parameter that controls the randomness of the output. Temperature values can range from 0.0 to 2.0. We can check the temperature of our current model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPndrGi2nP5j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "671b920c-da58-4156-aba9-68d9027e373f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "    if m.name == 'models/gemini-1.5-flash':\n",
        "        print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBdqPso3kamW"
      },
      "source": [
        "Let's initialize a new model with a temperature of 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GqupV-dYsvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "8e61affd-c4b4-48a7-c3f6-2165ecbd73c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Grab some friends, head to the Engineering Library for a late-night coding session fueled by pizza and caffeine, and brainstorm solutions to a challenging LeetCode problem. \n",
            "\n",
            "2. Grab some friends, head to the Engineering Library for a late-night coding session fueled by pizza and caffeine, and brainstorm solutions to a challenging LeetCode problem. \n",
            "\n",
            "3. Grab some friends, head to the Engineering Library for a late-night coding session fueled by pizza and caffeine, and brainstorm solutions to a challenging LeetCode problem. \n",
            "\n",
            "4. Grab some friends, head to the Engineering Library for a late-night coding session fueled by pizza and caffeine, and brainstorm solutions to a challenging LeetCode problem. \n",
            "\n",
            "5. Grab some friends, head to the Engineering Library for a late-night coding session fueled by pizza and caffeine, and brainstorm solutions to a challenging LeetCode problem. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "new_outputs = []\n",
        "low_temp_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0})\n",
        "for i in range(5):\n",
        "  response = low_temp_model.generate_content(prompt)\n",
        "  new_outputs.append(response.text)\n",
        "for index, sentence in enumerate(new_outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the outputs are the same! Finally Steve can be sure how to spend his night. Conversely, setting temperature to the max value of 2.0 would have the opposite effect, yielding more unpredictable responses."
      ],
      "metadata": {
        "id": "AZ78-JuN7XHi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5QDIG6cPbID"
      },
      "source": [
        "## Max Output Length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8MfrBmbPi5i"
      },
      "source": [
        "Let's say Steve wants his outputs to be below a certain length. In large language models, text is generated in tokens. For Gemini models, a token is equivalent to about 4 characters. 100 tokens are about 60-80 English words. He can set the `max_output_tokens` variable as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he-OfzBbhACQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e78e691-9fc8-4b03-b951-7e65e14ad9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grab some friends and head\n"
          ]
        }
      ],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "short_response_model = genai.GenerativeModel(model_name, generation_config={\"max_output_tokens\": 5})\n",
        "prompt = \"Help me choose a fun way to spend my Friday night as a CS major at Berkeley in a single one-sentence idea\"\n",
        "response = short_response_model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz31PRYSRMUt"
      },
      "source": [
        "Notice that this simply halts token generation at a fixed quantity and does not guarantee that the output is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4672af98ac57"
      },
      "source": [
        "## Token Count\n",
        "\n",
        "Let's say that Steve is being charged for every token that he inputs to Gemini.\n",
        "\n",
        "If Steve has billing enabled, the price of a paid request is controlled by the number of input and output tokens, so knowing how to count your tokens is important. As such, Steve might want to know the number of tokens in his prompt (input) before actually putting it into the model.\n",
        "\n",
        "Let's create a new instance of Gemini 1.5 Flash and set our prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "338fb9a6af78"
      },
      "outputs": [],
      "source": [
        "model_name = 'gemini-1.5-flash'\n",
        "token_n_model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0.0})\n",
        "poem_prompt = \"Write me a poem about Berkeley's campus\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = token_n_model.generate_content(poem_prompt)\n",
        "response.text"
      ],
      "metadata": {
        "id": "9WVlpLcDLjOs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "92e15a7b-b020-4c47-9a51-3bc26bcd14db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Where golden hills meet azure sky,\\nA campus sprawls, a vibrant sigh.\\nBerkeley, a name that echoes bold,\\nOf minds ablaze, stories untold.\\n\\nThe Campanile, a tower tall,\\nIts chimes resound, answering the call\\nOf students thronging, eager to learn,\\nIn classrooms bright, where knowledge burns.\\n\\nSather Gate, a grand archway,\\nLeads to paths where dreams hold sway.\\nFrom Sproul Plaza, a bustling scene,\\nTo Bancroft's charm, a tranquil green.\\n\\nThe scent of eucalyptus fills the air,\\nAs squirrels scamper, without a care.\\nThe Golden Bear, a mascot proud,\\nRoams the grounds, a spirit endowed.\\n\\nFrom Wheeler Hall, a history deep,\\nTo Doe Library, where secrets sleep.\\nThe Greek Theatre, a stage of grace,\\nWhere laughter rings, and time finds space.\\n\\nA tapestry of cultures, rich and bright,\\nA melting pot, day and night.\\nBerkeley, a haven for the free,\\nWhere minds ignite, and spirits flee.\\n\\nSo let us wander, explore, and dream,\\nIn this hallowed space, a vibrant stream.\\nFor Berkeley's heart, a beating drum,\\nA symphony of knowledge, yet to come. \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before generating a response, we can check how many tokens are in this prompt using the `.count_tokens` function of the model:"
      ],
      "metadata": {
        "id": "-1w3HL5nIyR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_token_count = token_n_model.count_tokens(poem_prompt)\n",
        "output_token_count = token_n_model.count_tokens(response.text)\n",
        "print(f'Tokens in prompt: {prompt_token_count} \\n Estimated tokens in output {output_token_count}')"
      ],
      "metadata": {
        "id": "lJ0V8H2dIf9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1c951198-e71c-40a8-adb2-232dfb9e0e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens in prompt: total_tokens: 9\n",
            " \n",
            " Estimated tokens in output total_tokens: 269\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}